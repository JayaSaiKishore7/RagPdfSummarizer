# ğŸ¤– RAG-Powered Document Question Answering App

An AI-powered Retrieval-Augmented Generation (RAG) application that allows users to **upload custom documents (PDFs)** and interactively **ask questions** about their content using **LLMs**, **vector embeddings**, and **natural language processing**.

---

## ğŸš€ Features

- ğŸ“„ Upload and manage PDF documents
- ğŸ§  Ask natural language questions based on document content
- ğŸ” Embedding-based document chunking and semantic retrieval using **FAISS**
- ğŸ’¬ Context-aware LLM responses via **OpenAI / HuggingFace Transformers**
- âš™ï¸ Powered by **LangChain** for modular RAG pipeline
- ğŸ“± Clean, responsive web UI built with Flask & HTML templates

---

## ğŸ§° Tech Stack

- **Python 3.8+*
- **Flask** (for web server and routing)
- **LangChain** (for chaining document retriever + LLM)
- **Vector DB:** FAISS (can be extended to Pinecone, ChromaDB, etc.)
- **LLM Providers:** OpenAI API / HuggingFace Transformers
- **PDF Parsing:** PyMuPDF / pdfminer
- **Env Handling:** `dotenv`

---

## Installation

1. Clone this repository:
```bash
git clone https://github.com/JayaSaiKishore7/RagPdfSummarizer
cd rag_application
```

2. Create a virtual environment and activate it:
```bash
python -m venv venv
# On Windows
venv\Scripts\activate
# On macOS/Linux
source venv/bin/activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Set up environment variables (if needed):
```bash
# Create .env file with required variables
```

## Usage

1. Start the application:
```bash
python app.py
```

2. Open your web browser and navigate to `http://localhost:5000`

3. Upload documents using the interface on the left panel

4. Ask questions about your documents in the question box at the bottom of the right panel

## Project Structure

    rag_application/
    â”‚
    â”œâ”€â”€ app.py                # Flask application entry point
    â”œâ”€â”€ rag_utils.py          # LangChain + RAG setup (retriever, prompt chain)
    â”œâ”€â”€ templates/
    â”‚   â””â”€â”€ index.html        # UI template
    â”œâ”€â”€ static/
    â”‚   â””â”€â”€ style.css         # Custom CSS
    â”œâ”€â”€ uploads/              # Uploaded PDF storage
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ .env

## How It Works (Behind the Scenes)
- PDF Upload: User uploads a document
- Chunking & Embedding: PDF is split into chunks & embedded using OpenAI/HuggingFace embeddings  
- Vector Storage: Embeddings are stored/retrieved using FAISS 
- Query Handling: User question is embedded and top relevant chunks are retrieved  
- LLM Generation: LLM generates a contextual, semantic answer

## Requirements

- Python 3.8+
- Flask
- Vector database (e.g., FAISS, Pinecone)
- LLM integration (e.g., OpenAI API)

